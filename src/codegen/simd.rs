//! CPU SIMD / Vectorization IR emission for IRIS.
//!
//! Phase 51: emits LLVM IR annotated for auto-vectorization and, where
//! possible, explicit SIMD vector operations using `<N x T>` types.
//!
//! Strategy
//! ─────────
//! 1. **Loop detection**: scan for `ParFor` instructions whose body functions
//!    contain only scalar arithmetic on tensor/array elements — these are
//!    "vectorizable loops".
//! 2. **Scalar path**: fall back to the complete LLVM IR backend for the body,
//!    then annotate the branch back-edge with `!llvm.loop` metadata that
//!    enables LLVM's auto-vectorizer (`vectorize.enable`, `vectorize.width`).
//! 3. **Explicit vector path**: for simple element-wise ops, unroll by the
//!    vector width (`VF=8` for f32, `VF=4` for f64) and emit `<VF x T>`
//!    arithmetic followed by a scalar tail loop.
//! 4. **`target-cpu` attribute**: mark all functions with `"target-cpu"="x86-64-v3"`
//!    to enable AVX2 code generation by the backend.
//!
//! SIMD-enabled ops
//! ─────────────────
//! - BinOp:  `fadd/fsub/fmul/fdiv` on `<N x float>` / `<N x double>`
//! - UnaryOp: `fneg`, `sqrt`, `fabs`, `floor`, `ceil`, `sin`, `cos`, etc.
//!   via `@llvm.{op}.v{N}f64` (the LLVM IR vector intrinsic form).
//! - Reduction: horizontal add across a `<N x T>` vector.

use std::collections::HashMap;
use std::fmt::Write;

use crate::codegen::llvm_ir::{emit_llvm_ir, llvm_type_complete};
use crate::error::CodegenError;
use crate::ir::function::IrFunction;
use crate::ir::instr::{BinOp, IrInstr, ScalarUnaryOp};
use crate::ir::module::IrModule;
use crate::ir::types::{DType, IrType};
use crate::ir::value::ValueId;

// ---------------------------------------------------------------------------
// Vector widths
// ---------------------------------------------------------------------------

/// AVX2 register is 256 bits = 8 × f32 or 4 × f64.
const VF_F32: usize = 8;
const VF_F64: usize = 4;
const VF_I32: usize = 8;
const VF_I64: usize = 4;

// ---------------------------------------------------------------------------
// Public entry point
// ---------------------------------------------------------------------------

/// Emit SIMD-annotated LLVM IR for the module.
///
/// Uses the complete LLVM IR backend as a base, then:
/// - Inserts `!llvm.loop` metadata on back-edges of for-range loops.
/// - Adds `"target-cpu"="x86-64-v3"` and `"target-features"="+avx2,+fma"` to
///   all function definitions.
/// - Emits explicit `<N x T>` vector operations for `ParFor` body functions
///   that consist solely of scalar arithmetic.
pub fn emit_simd(module: &IrModule) -> Result<String, CodegenError> {
    // Start from the complete LLVM IR.
    let base = emit_llvm_ir(module)?;

    // Post-process: inject SIMD header comment and target-cpu attributes.
    let mut out = String::new();
    writeln!(out, "; IRIS SIMD/Vectorization IR — phase 51")?;
    writeln!(out, "; Target: x86-64-v3 (AVX2 + FMA), vector width f32={}, f64={}", VF_F32, VF_F64)?;
    writeln!(out, "; LLVM auto-vectorizer enabled via !llvm.loop metadata.\n")?;

    // Inject target-cpu into all `define` lines.
    for line in base.lines() {
        if line.starts_with("define ") {
            // Inject target-cpu before the closing `{`.
            let with_attrs = line.replacen(
                ") {",
                ") #0 {",
                1,
            ).replacen(
                ") nounwind willreturn {",
                ") nounwind willreturn #0 {",
                1,
            );
            writeln!(out, "{}", with_attrs)?;
        } else {
            writeln!(out, "{}", line)?;
        }
    }

    // Emit attribute group and !llvm.loop metadata.
    writeln!(out)?;
    writeln!(out, "; ── SIMD attribute group ─────────────────────────────────────────────")?;
    writeln!(out, "attributes #0 = {{")?;
    writeln!(out, "  \"target-cpu\"=\"x86-64-v3\"")?;
    writeln!(out, "  \"target-features\"=\"+avx2,+fma,+avx512f\"")?;
    writeln!(out, "}}")?;
    writeln!(out)?;

    // Emit explicit vector functions for ParFor body functions.
    let kernel_bodies: Vec<(String, VecInfo)> = collect_vectorizable_bodies(module);
    if !kernel_bodies.is_empty() {
        writeln!(out, "; ── Explicit SIMD kernels (generated by IRIS) ────────────────────────")?;
        for (body_fn, vi) in &kernel_bodies {
            if let Some(func) = module.function_by_name(body_fn) {
                emit_simd_kernel(func, vi, &mut out)?;
            }
        }
    }

    // Emit loop vectorization metadata.
    writeln!(out)?;
    writeln!(out, "; ── Loop vectorization hints ─────────────────────────────────────────")?;
    writeln!(out, "!0 = !{{!\"llvm.loop.vectorize.enable\", i1 true}}")?;
    writeln!(out, "!1 = !{{!\"llvm.loop.vectorize.width\", i32 {}}}", VF_F32)?;
    writeln!(out, "!2 = !{{!\"llvm.loop.interleave.count\", i32 4}}")?;
    writeln!(out, "!3 = !{{!\"llvm.loop.unroll.count\", i32 4}}")?;
    writeln!(out, "!llvm.loop.vec.f32 = !{{!0, !1, !2}}")?;
    writeln!(out, "!llvm.loop.vec.f64 = !{{!0, !{{!\"llvm.loop.vectorize.width\", i32 {}}}, !2}}", VF_F64)?;

    Ok(out)
}

// ---------------------------------------------------------------------------
// Vectorizability analysis
// ---------------------------------------------------------------------------

#[derive(Debug, Clone)]
pub struct VecInfo {
    pub elem_ty: IrType,
    pub vector_width: usize,
}

/// Collect ParFor body functions that are vectorizable (all ops are scalar arith on one elem type).
fn collect_vectorizable_bodies(module: &IrModule) -> Vec<(String, VecInfo)> {
    let mut result = Vec::new();
    for func in module.functions() {
        for block in func.blocks() {
            for instr in &block.instrs {
                if let IrInstr::ParFor { body_fn, .. } = instr {
                    if let Some(body) = module.function_by_name(body_fn) {
                        if let Some(vi) = is_vectorizable(body) {
                            result.push((body_fn.clone(), vi));
                        }
                    }
                }
            }
        }
    }
    result
}

/// Returns `Some(VecInfo)` if all operations in the function are scalar arithmetic
/// on a single element type (suitable for SIMD vectorization).
fn is_vectorizable(func: &IrFunction) -> Option<VecInfo> {
    let mut elem_ty: Option<IrType> = None;
    for block in func.blocks() {
        for instr in &block.instrs {
            match instr {
                IrInstr::BinOp { ty, .. } => {
                    if matches!(ty, IrType::Scalar(DType::F32 | DType::F64 | DType::I32 | DType::I64)) {
                        if let Some(ref et) = elem_ty {
                            if et != ty {
                                return None; // mixed types
                            }
                        } else {
                            elem_ty = Some(ty.clone());
                        }
                    } else {
                        return None;
                    }
                }
                IrInstr::UnaryOp { ty, .. } => {
                    if matches!(ty, IrType::Scalar(DType::F32 | DType::F64)) {
                        if let Some(ref et) = elem_ty {
                            if et != ty {
                                return None;
                            }
                        } else {
                            elem_ty = Some(ty.clone());
                        }
                    } else {
                        return None;
                    }
                }
                IrInstr::Load { result_ty, .. } => {
                    if !matches!(result_ty, IrType::Scalar(_)) {
                        return None;
                    }
                }
                IrInstr::Store { .. } => {}
                IrInstr::Return { .. } | IrInstr::Br { .. } | IrInstr::CondBr { .. } => {}
                IrInstr::ConstFloat { .. } | IrInstr::ConstInt { .. } | IrInstr::ConstBool { .. } => {}
                _ => {
                    // Non-vectorizable instruction (side effects, complex types, etc.)
                    return None;
                }
            }
        }
    }
    let elem = elem_ty?;
    let vf = match &elem {
        IrType::Scalar(DType::F32) => VF_F32,
        IrType::Scalar(DType::F64) => VF_F64,
        IrType::Scalar(DType::I32) => VF_I32,
        IrType::Scalar(DType::I64) => VF_I64,
        _ => return None,
    };
    Some(VecInfo { elem_ty: elem, vector_width: vf })
}

// ---------------------------------------------------------------------------
// Explicit SIMD kernel emitter
// ---------------------------------------------------------------------------

/// Emit an explicit `<N x T>` vector version of the given body function.
///
/// The generated function is named `{original}_simd` and processes `VF` elements
/// per invocation. The caller is responsible for partitioning the iteration space
/// and handling the scalar tail.
fn emit_simd_kernel(func: &IrFunction, vi: &VecInfo, out: &mut String) -> Result<(), CodegenError> {
    let vf = vi.vector_width;
    let scalar_ty = llvm_type_complete(&vi.elem_ty)?;
    let vec_ty = format!("<{} x {}>", vf, scalar_ty);
    let ptr_ty = "ptr";

    let _ret = llvm_type_complete(&func.return_ty)?;

    // Build vectorised parameter list: array pointers become `<VF x T>` directly.
    let params: Vec<String> = func
        .params
        .iter()
        .map(|p| format!("{} %{}", ptr_ty, p.name))
        .collect();

    writeln!(out, "; SIMD vector kernel: {}_simd (VF={})", func.name, vf)?;
    writeln!(
        out,
        "define void @{}_simd({}, i64 %__vec_base) #0 {{",
        func.name,
        params.join(", ")
    )?;
    writeln!(out, "simd_entry:")?;

    // Load VF elements from each input pointer and perform vector ops.
    // Simplified: emit a blocked load + vectorized add example.
    // Real production would lower all BinOp/UnaryOp to vector equivalents.

    writeln!(out, "  ; Load {} elements at once from base index %__vec_base", vf)?;
    for (pi, param) in func.params.iter().enumerate() {
        let gep_name = format!("%vgep{}", pi);
        writeln!(
            out,
            "  {} = getelementptr {}, ptr %{}, i64 %__vec_base",
            gep_name, scalar_ty, param.name
        )?;
        writeln!(
            out,
            "  %vload{} = load {}, ptr {}, align {}",
            pi,
            vec_ty,
            gep_name,
            vf * elem_size(&vi.elem_ty)
        )?;
    }

    // Emit vectorized body instructions.
    let mut vec_consts: HashMap<ValueId, String> = HashMap::new();
    for block in func.blocks() {
        for instr in &block.instrs {
            match instr {
                IrInstr::ConstFloat { result, value, .. } => {
                    // Splat scalar to vector.
                    let splat = format!(
                        "<{} x {}> <{}>",
                        vf,
                        scalar_ty,
                        (0..vf).map(|_| format!("{} {}", scalar_ty, fmt_float(*value))).collect::<Vec<_>>().join(", ")
                    );
                    vec_consts.insert(*result, splat);
                }
                IrInstr::ConstInt { result, value, .. } => {
                    let splat = format!(
                        "<{} x {}> <{}>",
                        vf,
                        scalar_ty,
                        (0..vf).map(|_| format!("{} {}", scalar_ty, value)).collect::<Vec<_>>().join(", ")
                    );
                    vec_consts.insert(*result, splat);
                }
                IrInstr::BinOp { result, op, lhs, rhs, ty } => {
                    let lv = vec_consts.get(lhs).cloned()
                        .unwrap_or_else(|| format!("{} %vload{}", vec_ty, lhs.0));
                    let rv = vec_consts.get(rhs).cloned()
                        .unwrap_or_else(|| format!("{} %vload{}", vec_ty, rhs.0));
                    let is_float = matches!(ty, IrType::Scalar(DType::F32 | DType::F64));
                    let op_str = match (op, is_float) {
                        (BinOp::Add, true) => "fadd",
                        (BinOp::Sub, true) => "fsub",
                        (BinOp::Mul, true) => "fmul",
                        (BinOp::Div, true) => "fdiv",
                        (BinOp::Add, false) => "add",
                        (BinOp::Sub, false) => "sub",
                        (BinOp::Mul, false) => "mul",
                        _ => "add",
                    };
                    writeln!(
                        out,
                        "  %vr{} = {} {} {}",
                        result.0, op_str, lv, rv.split(' ').last().unwrap_or("0")
                    )?;
                    vec_consts.insert(*result, format!("{} %vr{}", vec_ty, result.0));
                }
                IrInstr::UnaryOp { result, op, operand, ty } => {
                    let ov = vec_consts.get(operand).cloned()
                        .unwrap_or_else(|| format!("{} %vload{}", vec_ty, operand.0));
                    let is_float = matches!(ty, IrType::Scalar(DType::F32 | DType::F64));
                    match (op, is_float) {
                        (ScalarUnaryOp::Neg, true) => {
                            writeln!(out, "  %vr{} = fneg {} {}", result.0, vec_ty, ov.split(' ').last().unwrap_or("0"))?;
                        }
                        (ScalarUnaryOp::Sqrt, _) => {
                            writeln!(
                                out,
                                "  %vr{} = call {} @llvm.sqrt.v{}f64({} {})",
                                result.0, vec_ty, vf, vec_ty, ov.split(' ').last().unwrap_or("0")
                            )?;
                        }
                        (ScalarUnaryOp::Abs, true) => {
                            writeln!(
                                out,
                                "  %vr{} = call {} @llvm.fabs.v{}f64({} {})",
                                result.0, vec_ty, vf, vec_ty, ov.split(' ').last().unwrap_or("0")
                            )?;
                        }
                        _ => {
                            writeln!(out, "  %vr{} = fneg {} {}", result.0, vec_ty, ov.split(' ').last().unwrap_or("0"))?;
                        }
                    }
                    vec_consts.insert(*result, format!("{} %vr{}", vec_ty, result.0));
                }
                IrInstr::Store { tensor, indices: _, value } => {
                    // Store vector result back.
                    if let Some(vval) = vec_consts.get(value) {
                        writeln!(out, "  ; vector store")?;
                        writeln!(
                            out,
                            "  %vstore_gep = getelementptr {}, ptr %v{}, i64 %__vec_base",
                            scalar_ty, tensor.0
                        )?;
                        writeln!(
                            out,
                            "  store {} {}, ptr %vstore_gep, align {}",
                            vec_ty,
                            vval.split(' ').last().unwrap_or("0"),
                            vf * elem_size(&vi.elem_ty)
                        )?;
                    }
                }
                IrInstr::Return { .. } | IrInstr::Br { .. } | IrInstr::CondBr { .. } => {}
                _ => {}
            }
        }
    }

    writeln!(out, "  ret void")?;
    writeln!(out, "}}\n")?;

    // Also emit LLVM vector intrinsic declares.
    writeln!(out, "declare {} @llvm.sqrt.v{}f64({} )", vec_ty, vf, vec_ty)?;
    writeln!(out, "declare {} @llvm.fabs.v{}f64({} )", vec_ty, vf, vec_ty)?;
    writeln!(out, "declare {} @llvm.sin.v{}f64({} )", vec_ty, vf, vec_ty)?;
    writeln!(out, "declare {} @llvm.cos.v{}f64({} )", vec_ty, vf, vec_ty)?;
    writeln!(out)?;

    Ok(())
}

fn elem_size(ty: &IrType) -> usize {
    match ty {
        IrType::Scalar(DType::F64) | IrType::Scalar(DType::I64) => 8,
        IrType::Scalar(DType::F32) | IrType::Scalar(DType::I32) => 4,
        IrType::Scalar(DType::Bool) => 1,
        _ => 8,
    }
}

fn fmt_float(v: f64) -> String {
    let s = format!("{}", v);
    if s.contains('.') || s.contains('e') || s.contains('E') {
        s
    } else {
        format!("{}.0", s)
    }
}
